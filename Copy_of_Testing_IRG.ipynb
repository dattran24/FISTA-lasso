{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Testing_IRG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMW2uQPRpK19DciT7IF2DtM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/he9180/FISTA-lasso/blob/main/Copy_of_Testing_IRG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical experiments for IRG methods. First we import some necessary packages."
      ],
      "metadata": {
        "id": "N9FhlaqAtYQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import rosen\n",
        "from scipy.optimize import rosen_der\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "6Ax2qiyZnPfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create a function generating a fix random vector $x\\in \\mathbb{R}^{\\rm size}$, where $x_i\\in [{\\rm low},{\\rm high}]$."
      ],
      "metadata": {
        "id": "iYmhlujftsxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rand(low, high, size, random_state=42, endpoint=True):\n",
        "    rng = np.random.default_rng(random_state);\n",
        "    return rng.integers(low, high, size=size, endpoint=endpoint);\n",
        " #rand(-1, 1, 10);   "
      ],
      "metadata": {
        "id": "mFKkHDg0nVez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dixon(X):\n",
        "  d = len(X);\n",
        "  f = (X[0] - 1) **2 + np.sum([(i+1) * (2*X[i]**2 - X[i-1])**2 for i in range(1, d)]);\n",
        "  return f\n",
        "def dixon_der(X):\n",
        "  d=len(X);\n",
        "  term1=np.zeros(d);\n",
        "  term1[0]=2*(X[0]-1);\n",
        "  for i in range(1,d):\n",
        "    term1[i]=term1[i]+(i+1)*8*X[i]*(2*(X[i]**2)-X[i-1]);\n",
        "    term1[i-1]=term1[i-1]-2*(i+1)*(2*(X[i]**2)-X[i-1]);\n",
        "  return term1\n",
        "def dixon_sol(d):\n",
        "  x=np.zeros(d);\n",
        "  x[0]=1;\n",
        "  for i in range(1,d):\n",
        "    x[i]=np.sqrt(0.5*x[i-1]);\n",
        "  return x"
      ],
      "metadata": {
        "id": "LKNUFQBfvPA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we provide the code for steepest descent method."
      ],
      "metadata": {
        "id": "1MaiFRtGuKnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def STD(x_init,para,option,prob):\n",
        "  print('                               Steepest descent method ')\n",
        "  x=x_init;\n",
        "  beta=para.beta;\n",
        "  gamma=para.gamma;\n",
        "  fval=prob.fval;\n",
        "  grad=prob.grad;\n",
        "  k=1;\n",
        "  if hasattr(option, 'norm_grad'):\n",
        "    norm_grad=option.norm_grad;\n",
        "    while np.linalg.norm(grad(x))> norm_grad:\n",
        "      if option.display==1:\n",
        "        if k>1:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|Stepsize:',t);\n",
        "        else:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)));\n",
        "      g=grad(x);\n",
        "      d=-g;\n",
        "      t=1;\n",
        "      while fval(x+t*d)>fval(x)-beta*t*(np.linalg.norm(d))**2:\n",
        "        t=t*gamma;\n",
        "      x=x+t*d;\n",
        "      k=k+1;\n",
        "    print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)));\n",
        "  return x;"
      ],
      "metadata": {
        "id": "g9Rfq9sVhW8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we introduce the code for IRG method with backtracking lineseach."
      ],
      "metadata": {
        "id": "NIdeuJuduRNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IRG_backtracking(x_init,para,option,prob):\n",
        "  print('                               IRG backtracking ');\n",
        "  x=x_init;\n",
        "  m=len(x);\n",
        "  er=rand(-1,1,m);\n",
        "  error=er/np.linalg.norm(er);\n",
        "  beta=para.beta;\n",
        "  gamma=para.gamma;\n",
        "  theta=para.theta;\n",
        "  mu=para.mu;\n",
        "  eps=para.eps;\n",
        "  r=para.r;\n",
        "  fval=prob.fval;\n",
        "  grad=prob.grad;\n",
        "  count=0;\n",
        "  k=1;\n",
        "  if hasattr(option, 'norm_grad'):\n",
        "    norm_grad=option.norm_grad;\n",
        "    while np.linalg.norm(grad(x))> norm_grad:\n",
        "      if option.display==1:\n",
        "        if k>1:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|Stepsize:',t);\n",
        "        else:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)));\n",
        "      g=grad(x)+0.5*min(eps,1/(k+1))*error;\n",
        "      if np.linalg.norm(g)>r+eps:\n",
        "        d=-(np.linalg.norm(g)-eps)/np.linalg.norm(g)*g;\n",
        "        t=1;\n",
        "        while fval(x+t*d)>fval(x)-beta*t*np.linalg.norm(d)**2:\n",
        "          t=t*gamma;\n",
        "        x=x+t*d;\n",
        "        nulls=0;\n",
        "      else:\n",
        "        eps=eps*theta;\n",
        "        r=r*mu;\n",
        "        nulls=1;\n",
        "        count=count+1;\n",
        "      k=k+1;\n",
        "    print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|eps:',eps,'|error:',0.5*min(eps,1/(k+1)));\n",
        "  print(' IRG-backtracking non-null iterations',k-count)\n",
        "  return x;"
      ],
      "metadata": {
        "id": "3k0-1Noyld2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we introduce the code for IRG method with unbounded backtracking lineseach."
      ],
      "metadata": {
        "id": "BXkYDtxkua0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IRG_unbounded(x_init,para,option,prob):\n",
        "  print('                               IRG-unbounded ');\n",
        "  x=x_init;\n",
        "  m=len(x);\n",
        "  er=rand(-1,1,m);\n",
        "  error=er/np.linalg.norm(er);\n",
        "  beta=para.beta;\n",
        "  gamma=para.gamma;\n",
        "  theta=para.theta;\n",
        "  mu=para.mu;\n",
        "  eps=para.eps;\n",
        "  r=para.r;\n",
        "  fval=prob.fval;\n",
        "  grad=prob.grad;\n",
        "  count=0;\n",
        "  k=1;\n",
        "  if hasattr(option, 'norm_grad'):\n",
        "    norm_grad=option.norm_grad;\n",
        "    while np.linalg.norm(grad(x))> norm_grad:\n",
        "      if option.display==1:\n",
        "        if k>1:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|Stepsize:',t);\n",
        "        else:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)));\n",
        "      g=grad(x)+0.5*min(eps,1/(k+1))*error;\n",
        "      if np.linalg.norm(g)>r+eps:\n",
        "        d=-(np.linalg.norm(g)-eps)/np.linalg.norm(g)*g;\n",
        "        t=1;\n",
        "        if fval(x+d)>fval(x)-beta*np.linalg.norm(d)**2:\n",
        "          while fval(x+t*d)>fval(x)-beta*t*np.linalg.norm(d)**2:\n",
        "            t=t*gamma;\n",
        "        else:\n",
        "          while fval(x+t*d)<=fval(x)-beta*t*np.linalg.norm(d)**2:\n",
        "            t=t/gamma;\n",
        "          t=t*gamma;\n",
        "        x=x+t*d;\n",
        "        nulls=0;\n",
        "      else:\n",
        "        eps=eps*theta;\n",
        "        r=r*mu;\n",
        "        nulls=1;\n",
        "        count=count+1;\n",
        "      k=k+1;\n",
        "    print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|eps:',eps,'|error:',0.5*min(eps,1/(k+1)));\n",
        "  print(' IRG-unbounded non-null iterations',k-count)\n",
        "  return x;"
      ],
      "metadata": {
        "id": "GnVJbj1trTuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we introduce the code for RG method with unbounded backtracking lineseach."
      ],
      "metadata": {
        "id": "5Mw8D-_VudPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RG_unbounded(x_init,para,option,prob):\n",
        "  print('                               RG-unbounded ');\n",
        "  x=x_init;\n",
        "  m=len(x);\n",
        "  beta=para.beta;\n",
        "  gamma=para.gamma;\n",
        "  theta=para.theta;\n",
        "  mu=para.mu;\n",
        "  eps=para.eps;\n",
        "  r=para.r;\n",
        "  fval=prob.fval;\n",
        "  grad=prob.grad;\n",
        "  count=0;\n",
        "  k=1;\n",
        "  if hasattr(option, 'norm_grad'):\n",
        "    norm_grad=option.norm_grad;\n",
        "    while np.linalg.norm(grad(x))> norm_grad:\n",
        "      if option.display==1:\n",
        "        if k>1:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|Stepsize:',t);\n",
        "        else:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)));\n",
        "      g=grad(x);\n",
        "      if np.linalg.norm(g)>r+eps:\n",
        "        d=-(np.linalg.norm(g)-eps)/np.linalg.norm(g)*g;\n",
        "        t=1;\n",
        "        if fval(x+d)>fval(x)-beta*np.linalg.norm(d)**2:\n",
        "          while fval(x+t*d)>fval(x)-beta*t*np.linalg.norm(d)**2:\n",
        "            t=t*gamma;\n",
        "        else:\n",
        "          while fval(x+t*d)<=fval(x)-beta*t*np.linalg.norm(d)**2:\n",
        "            t=t/gamma;\n",
        "          t=t*gamma;\n",
        "        x=x+t*d;\n",
        "        nulls=0;\n",
        "      else:\n",
        "        eps=eps*theta;\n",
        "        r=r*mu;\n",
        "        nulls=1;\n",
        "        count=count+1;\n",
        "      k=k+1;\n",
        "    print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|eps:',eps,'|error:',0.5*min(eps,1/(k+1)));\n",
        "  print(' RG-unbounded non-null iterations',k-count)\n",
        "  return x;"
      ],
      "metadata": {
        "id": "f_3GBIrisZEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we introduce the code for RG method with backtracking lineseach."
      ],
      "metadata": {
        "id": "ASh8mdZ0uiSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RG_backtracking(x_init,para,option,prob):\n",
        "  print('                               RG backtracking ');\n",
        "  x=x_init;\n",
        "  m=len(x);\n",
        "  beta=para.beta;\n",
        "  gamma=para.gamma;\n",
        "  theta=para.theta;\n",
        "  mu=para.mu;\n",
        "  eps=para.eps;\n",
        "  r=para.r;\n",
        "  fval=prob.fval;\n",
        "  grad=prob.grad;\n",
        "  count=0;\n",
        "  k=1;\n",
        "  if hasattr(option, 'norm_grad'):\n",
        "    norm_grad=option.norm_grad;\n",
        "    while np.linalg.norm(grad(x))> norm_grad:\n",
        "      if option.display==1:\n",
        "        if k>1:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|Stepsize:',t);\n",
        "        else:\n",
        "          print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)));\n",
        "      g=grad(x);\n",
        "      if np.linalg.norm(g)>r+eps:\n",
        "        d=-(np.linalg.norm(g)-eps)/np.linalg.norm(g)*g;\n",
        "        t=1;\n",
        "        while fval(x+t*d)>fval(x)-beta*t*np.linalg.norm(d)**2:\n",
        "          t=t*gamma;\n",
        "        x=x+t*d;\n",
        "        nulls=0;\n",
        "      else:\n",
        "        eps=eps*theta;\n",
        "        r=r*mu;\n",
        "        nulls=1;\n",
        "        count=count+1;\n",
        "      k=k+1;\n",
        "    print(' Iter:',k,'|Value:',fval(x),'|Norm grad:',np.linalg.norm(grad(x)),'|eps:',eps,'|error:',0.5*min(eps,1/(k+1)));\n",
        "  print(' RG-backtracking non-null iterations',k-count)\n",
        "  return x;"
      ],
      "metadata": {
        "id": "duCKrO-DquXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we present numerical experiments. First we choose some linesearch parameters and running options."
      ],
      "metadata": {
        "id": "xQNKz2Hkuo5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT0P45dGrDFN",
        "outputId": "02b1b30a-ce4f-4642-ebe3-a7dd5dda6dbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------Testing with Rosenbrock function 20--------------------------\n",
            "                               Steepest descent method \n",
            " Iter: 4907 |Value: 9.49317533064536e-05 |Norm grad: 0.009995476197742978\n",
            "                               IRG backtracking \n",
            " Iter: 3651 |Value: 9.376640950370217e-05 |Norm grad: 0.00989642102582078 |eps: 0.003989613314880596\n",
            " IRG-backtracking non-null iterations 3631\n",
            "                               RG backtracking \n",
            " Iter: 3650 |Value: 9.408970058741499e-05 |Norm grad: 0.009937509544784198 |eps: 0.003989613314880596\n",
            " RG-backtracking non-null iterations 3630\n",
            "                               IRG-unbounded \n",
            " Iter: 3651 |Value: 9.376640950370217e-05 |Norm grad: 0.00989642102582078 |eps: 0.003989613314880596\n",
            " IRG-unbounded non-null iterations 3631\n",
            "                               RG-unbounded \n",
            " Iter: 3650 |Value: 9.408970058741499e-05 |Norm grad: 0.009937509544784198 |eps: 0.003989613314880596\n",
            " RG-unbounded non-null iterations 3630\n"
          ]
        }
      ],
      "source": [
        "class para():\n",
        "  beta=0.7;\n",
        "  gamma=0.5;\n",
        "  theta=0.7;\n",
        "  mu=0.7;\n",
        "  eps=5;\n",
        "  r=5;\n",
        "class option():\n",
        "  norm_grad=0.01;\n",
        "  display=0;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we test with Rosenbrock function"
      ],
      "metadata": {
        "id": "IaSCv8F0u57R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------Testing with Rosenbrock function 20--------------------------');\n",
        "x_init = np.zeros(20);\n",
        "class prob():\n",
        "  fval=rosen;\n",
        "  grad=rosen_der;\n",
        "STD(x_init,para,option,prob);\n",
        "IRG_backtracking(x_init,para,option,prob);\n",
        "RG_backtracking(x_init,para,option,prob);\n",
        "IRG_unbounded(x_init,para,option,prob);\n",
        "RG_unbounded(x_init,para,option,prob);\n",
        "print('----------------------Testing with Rosenbrock function 500--------------------------');\n",
        "x_init = np.zeros(500);\n",
        "class prob():\n",
        "  fval=rosen;\n",
        "  grad=rosen_der;\n",
        "STD(x_init,para,option,prob);\n",
        "IRG_backtracking(x_init,para,option,prob);\n",
        "RG_backtracking(x_init,para,option,prob);\n",
        "IRG_unbounded(x_init,para,option,prob);\n",
        "RG_unbounded(x_init,para,option,prob);\n",
        "print('----------------------Testing with Rosenbrock function 2000--------------------------');\n",
        "x_init = np.zeros(2000);\n",
        "class prob():\n",
        "  fval=rosen;\n",
        "  grad=rosen_der;\n",
        "STD(x_init,para,option,prob);\n",
        "IRG_backtracking(x_init,para,option,prob);\n",
        "RG_backtracking(x_init,para,option,prob);\n",
        "IRG_unbounded(x_init,para,option,prob);\n",
        "RG_unbounded(x_init,para,option,prob);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFM4b9ziu7TH",
        "outputId": "ada506d8-2f7b-46fb-9889-8b255c7afbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------Testing with Rosenbrock function 20--------------------------\n",
            "                               Steepest descent method \n",
            " Iter: 4907 |Value: 9.49317533064536e-05 |Norm grad: 0.009995476197742978\n",
            "                               IRG backtracking \n",
            " Iter: 3651 |Value: 9.376640950370217e-05 |Norm grad: 0.00989642102582078 |eps: 0.003989613314880596 |error: 0.00013691128148959474\n",
            " IRG-backtracking non-null iterations 3631\n",
            "                               RG backtracking \n",
            " Iter: 3650 |Value: 9.408970058741499e-05 |Norm grad: 0.009937509544784198 |eps: 0.003989613314880596 |error: 0.0001369487811558477\n",
            " RG-backtracking non-null iterations 3630\n",
            "                               IRG-unbounded \n",
            " Iter: 3651 |Value: 9.376640950370217e-05 |Norm grad: 0.00989642102582078 |eps: 0.003989613314880596 |error: 0.00013691128148959474\n",
            " IRG-unbounded non-null iterations 3631\n",
            "                               RG-unbounded \n",
            " Iter: 3650 |Value: 9.408970058741499e-05 |Norm grad: 0.009937509544784198 |eps: 0.003989613314880596 |error: 0.0001369487811558477\n",
            " RG-unbounded non-null iterations 3630\n",
            "----------------------Testing with Rosenbrock function 500--------------------------\n",
            "                               Steepest descent method \n",
            " Iter: 46135 |Value: 9.342740248632365e-05 |Norm grad: 0.00990594414508587\n",
            "                               IRG backtracking \n",
            " Iter: 59611 |Value: 9.218238161754006e-05 |Norm grad: 0.009877249731904687 |eps: 0.003989613314880596 |error: 8.387572971884856e-06\n",
            " IRG-backtracking non-null iterations 59591\n",
            "                               RG backtracking \n",
            " Iter: 59604 |Value: 9.196896777195537e-05 |Norm grad: 0.009863579443289897 |eps: 0.003989613314880596 |error: 8.388558006878617e-06\n",
            " RG-backtracking non-null iterations 59584\n",
            "                               IRG-unbounded \n",
            " Iter: 59611 |Value: 9.218238161754006e-05 |Norm grad: 0.009877249731904687 |eps: 0.003989613314880596 |error: 8.387572971884856e-06\n",
            " IRG-unbounded non-null iterations 59591\n",
            "                               RG-unbounded \n",
            " Iter: 59604 |Value: 9.196896777195537e-05 |Norm grad: 0.009863579443289897 |eps: 0.003989613314880596 |error: 8.388558006878617e-06\n",
            " RG-unbounded non-null iterations 59584\n",
            "----------------------Testing with Rosenbrock function 2000--------------------------\n",
            "                               Steepest descent method \n",
            " Iter: 175062 |Value: 9.440549432952922e-05 |Norm grad: 0.009956700168496807\n",
            "                               IRG backtracking \n",
            " Iter: 234008 |Value: 8.826390251925907e-05 |Norm grad: 0.00961833030965523 |eps: 0.003989613314880596 |error: 2.1366699571384005e-06\n",
            " IRG-backtracking non-null iterations 233988\n",
            "                               RG backtracking \n",
            " Iter: 233985 |Value: 8.923565396440537e-05 |Norm grad: 0.009682481207193575 |eps: 0.003989613314880596 |error: 2.1368799842725634e-06\n",
            " RG-backtracking non-null iterations 233965\n",
            "                               IRG-unbounded \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we test with Dixon & Price function"
      ],
      "metadata": {
        "id": "Z6NRlVL53mUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('----------------------Testing with Dixon&Price function 20--------------------------');\n",
        "x_init = np.ones(20);\n",
        "class prob():\n",
        "  fval=dixon;\n",
        "  grad=dixon_der;\n",
        "STD(x_init,para,option,prob);\n",
        "IRG_backtracking(x_init,para,option,prob);\n",
        "RG_backtracking(x_init,para,option,prob);\n",
        "IRG_unbounded(x_init,para,option,prob);\n",
        "RG_unbounded(x_init,para,option,prob);\n",
        "print('----------------------Testing with Dixon&Price function 500--------------------------');\n",
        "x_init = np.ones(500);\n",
        "class prob():\n",
        "  fval=dixon;\n",
        "  grad=dixon_der;\n",
        "STD(x_init,para,option,prob);\n",
        "IRG_backtracking(x_init,para,option,prob);\n",
        "RG_backtracking(x_init,para,option,prob);\n",
        "IRG_unbounded(x_init,para,option,prob);\n",
        "RG_unbounded(x_init,para,option,prob);\n",
        "print('----------------------Testing with Dixon&Price function 2000--------------------------');\n",
        "x_init = np.ones(2000);\n",
        "class prob():\n",
        "  fval=dixon;\n",
        "  grad=dixon_der;\n",
        "STD(x_init,para,option,prob);\n",
        "IRG_backtracking(x_init,para,option,prob);\n",
        "RG_backtracking(x_init,para,option,prob);\n",
        "IRG_unbounded(x_init,para,option,prob);\n",
        "RG_unbounded(x_init,para,option,prob);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq6AqBBhylva",
        "outputId": "66d43a74-a96b-450b-cbf9-da4d045d25d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------Testing with Dixon&Price function 20--------------------------\n",
            "                               Steepest descent method \n",
            " Iter: 1928 |Value: 2.7790504935867027e-05 |Norm grad: 0.009946903078270647\n",
            "                               IRG backtracking \n",
            " Iter: 1004 |Value: 2.710264941790442e-05 |Norm grad: 0.009959857961855707 |eps: 0.003989613314880596\n",
            " IRG-backtracking non-null iterations 984\n",
            "                               RG backtracking \n",
            " Iter: 998 |Value: 2.7556184775233842e-05 |Norm grad: 0.009890570748953052 |eps: 0.003989613314880596\n",
            " RG-backtracking non-null iterations 978\n",
            "                               IRG-unbounded \n",
            " Iter: 1004 |Value: 2.710264941790442e-05 |Norm grad: 0.009959857961855707 |eps: 0.003989613314880596\n",
            " IRG-unbounded non-null iterations 984\n",
            "                               RG-unbounded \n",
            " Iter: 998 |Value: 2.7556184775233842e-05 |Norm grad: 0.009890570748953052 |eps: 0.003989613314880596\n",
            " RG-unbounded non-null iterations 978\n"
          ]
        }
      ]
    }
  ]
}